{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5da437-26fd-44b6-bc1f-a4ff4b8c81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from ketos.data_handling import selection_table as sl\n",
    "from ketos.audio.audio_loader import  AudioFrameLoader, FrameStepper, audio_repres_dict\n",
    "from ketos.neural_networks import load_model_file\n",
    "from ketos.neural_networks.resnet import ResNetInterface\n",
    "from ketos.neural_networks.dev_utils.detection import process, process_audio_loader, save_detections, merge_overlapping_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef1776-bdc8-4375-b54c-0432f9dd9cd4",
   "metadata": {},
   "source": [
    "# File paths and annotation load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf90f901-11a3-44da-ab4b-0b99c0fd1847",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_storage_root_path='/home/sadman/projects/ctb-ruthjoy/SRKW/'\n",
    "detection_file_path='../../model_detections/'\n",
    "annot_file_path='../../annotations/test/' # Path where test annotations are stored\n",
    "\n",
    "number_of_classes = 3\n",
    "\n",
    "# =============================================================\n",
    "# Uncomment one file_name to check the performance of the model\n",
    "file_name='jasco_boundary_pass.csv'\n",
    "# file_name='jasco_roberts_bank.csv'\n",
    "# file_name='onc_barkley_canyon.csv'\n",
    "# file_name='orcasound.csv'\n",
    "# file_name='superpod_lime_kiln.csv'\n",
    "# file_name='onc_barkley_canyon_test_multiclass.csv'\n",
    "# file_name='jasco_malahat_vfpa_test_multiclass.csv'\n",
    "# =============================================================\n",
    "\n",
    "# Fetch the test dataset name for further use\n",
    "file_name_except_extension=file_name[0:file_name.rindex('.')]\n",
    "\n",
    "# Firstly, load the corresponding annotation file\n",
    "annot_file_name=file_name_except_extension+'_annot.csv'\n",
    "annot_path_each_file=annot_file_path+annot_file_name\n",
    "annot_df=pd.read_csv(annot_path_each_file, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08675f-1a74-4afe-95a0-d54161d3d51a",
   "metadata": {},
   "source": [
    "# Load previously saved model detection file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130d2f67-233d-4a53-8844-50e5a13f6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previously saved model detection file\n",
    "model_detection_filename='detections_'+file_name_except_extension+'.csv'\n",
    "model_detections_df=pd.read_csv(detection_file_path+model_detection_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97be12db-4337-4d0e-98c1-3bb9536e9651",
   "metadata": {},
   "source": [
    "# Annotation pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d5c9db-cbd2-4121-809b-0a6fcc54e8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing_files(dataframe):\n",
    "    \"\"\" Save the detections to a csv file\n",
    "\n",
    "        Args:\n",
    "            dataframe: pandas.DataFrame\n",
    "                List of files for testing a detector\n",
    "                \n",
    "        Returns: \n",
    "            dataframe : pandas.DataFrame\n",
    "                Filtered dataframe after removing the files that do not exist\n",
    "    \"\"\"\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if(os.path.isfile(row['filename'])==False):\n",
    "            print(row['filename'], \" not found\")\n",
    "            dataframe = dataframe.drop([index])\n",
    "    return dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b947d-133c-46f9-9a3f-d4a38d53e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending audio folder location to each files\n",
    "annot_df['filename'] = audio_storage_root_path + annot_df['filename'].astype(str)\n",
    "\n",
    "# From annotations dataframe, remvoe all files which does not appear in the detection dataframe\n",
    "annot_unique_filenames=annot_df['filename'].unique()\n",
    "detections_unique_filenames=model_detections_df['filename'].unique()\n",
    "\n",
    "# Remove any extra files that are in annotation dataframe but not in detection dataframe\n",
    "extra_files_to_ignore=[]\n",
    "for file in annot_unique_filenames:\n",
    "    if(file not in detections_unique_filenames):\n",
    "        extra_files_to_ignore.append(file)\n",
    "annot_df = annot_df[~annot_df['filename'].isin(extra_files_to_ignore)]\n",
    "\n",
    "# Now, remove any extra files that are in detection dataframe but not in annotation dataframe\n",
    "extra_files_to_ignore=[]\n",
    "for file in detections_unique_filenames:\n",
    "    if(file not in annot_unique_filenames):\n",
    "        extra_files_to_ignore.append(file)\n",
    "model_detections_df = model_detections_df[~model_detections_df['filename'].isin(extra_files_to_ignore)]\n",
    "\n",
    "print(\"Number of files BEFORE removing missing files:\", len(model_detections_df))\n",
    "# Make sure each file actually exists in the cedar, if not, then remove from dataframe\n",
    "annot_df = remove_missing_files(annot_df)\n",
    "print(\"Number of files AFTER removing missing files:\", len(model_detections_df))\n",
    "\n",
    "# Convert start/end time columns to numerical values\n",
    "annot_df[\"start\"] = annot_df.start.astype(float)\n",
    "annot_df[\"end\"] = annot_df.end.astype(float)\n",
    "annot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a9e3c-0d93-4cb0-9dfc-57f4ae5d6ccf",
   "metadata": {},
   "source": [
    "# Some utility functions to re-format annotations and detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffab5cf-e083-4ab5-aad5-2b79cabcd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_annot_by_file(df_annot):\n",
    "    \"\"\" Store the annotations of each file into a dictionary\n",
    "\n",
    "        Args:\n",
    "            df_annot: pandas.DataFrame\n",
    "                DataFrame containing annotations\n",
    "                \n",
    "        Returns: \n",
    "            annot_by_file_dict: dict\n",
    "                Separated annotations of each file by filename\n",
    "\n",
    "    \"\"\"\n",
    "    annot_by_file_dict=dict()\n",
    "    unique_filenames=df_annot['filename'].unique()\n",
    "    for filename in unique_filenames:\n",
    "        target_matched_annot_df = df_annot[df_annot['filename'].str.find(filename) != -1]\n",
    "        annot_by_file_dict[filename]=target_matched_annot_df\n",
    "    return annot_by_file_dict\n",
    "\n",
    "def check_overlap(start, end, annotation_df, search_class_labels):\n",
    "    \"\"\" Check if a selection has overlap in the annotation table of a specific file\n",
    "     \n",
    "        Args:\n",
    "            start: float\n",
    "                Selection start time.\n",
    "            end: float\n",
    "                Selection end time.\n",
    "            annotation_df: pandas DataFrame\n",
    "                Annotation table.\n",
    "            search_class_label: int\n",
    "                Class label to search in the annotation table.\n",
    "\n",
    "        Returns:\n",
    "            overlap_found: bool\n",
    "                Returns True if there is overlap found, otherwise, returns False if the specifed selection does not match in the annotation dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    overlap_found = False\n",
    "    for annot_index, annot_row in annotation_df.iterrows():\n",
    "        if((annot_row['start'] <= start <= annot_row['end']) or \n",
    "           (annot_row['start'] <= end <= annot_row['end']) or\n",
    "           (start <= annot_row['start'] and end >= annot_row['end']) and\n",
    "           annot_row['label'] in search_class_labels):\n",
    "            overlap_found = True\n",
    "            break\n",
    "    return overlap_found\n",
    "\n",
    "def validate_annot_detections_serial(annot_df, detection_df):\n",
    "    \"\"\" Function to check if the annotation and detection serial is matching (filename and start time)\n",
    "        It also prints the row index if any row of the annot_df and detection_df doesn't match.\n",
    "\n",
    "        The serial of both dataframe is required to be same so that we can easily use the scikit-learn \n",
    "        packages for performance measurements.\n",
    "     \n",
    "        Args:\n",
    "            annot_df: pandas DataFrame\n",
    "                Annotation table.\n",
    "            detection_df: pandas DataFrame\n",
    "                Model detection dataframe.\n",
    "\n",
    "        Returns:\n",
    "            valid_flag: bool\n",
    "                Returns True if both dataframes are in the same serial, Else returns False.\n",
    "\n",
    "    \"\"\"\n",
    "    valid_flag=True\n",
    "    for i in range(len(annot_df['filename'].values)):\n",
    "        if(((annot_df['filename'].values)[i]!=(detection_df['filename'].values)[i]) or \n",
    "           ((annot_df['start'].values)[i]!=(detection_df['start'].values)[i])):\n",
    "            print(\"Not equal at index \", i)\n",
    "            valid_flag=False\n",
    "    return valid_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea881aef-7202-4440-8d53-506e6ba57fd6",
   "metadata": {},
   "source": [
    "# Get all files' duration in time (sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9c6a68-6846-49bf-8fbc-01854b175ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all files' duration in time\n",
    "unique_filenames=annot_df['filename'].unique()\n",
    "duration_list=[]\n",
    "for filename in unique_filenames:\n",
    "    duration_list.append(librosa.get_duration(filename=filename))\n",
    "    \n",
    "target_files_with_len = pd.DataFrame({'filename':unique_filenames, \n",
    "                                      'duration':duration_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b25fdbd-0c6d-4f67-9286-94ec7c2715ea",
   "metadata": {},
   "source": [
    "# Standardize annotations and segment annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc766ac2-af24-4c31-a077-16ad7e9734b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length, segment_step = 5.0, 5.0\n",
    "\n",
    "if(file_name_except_extension=='onc_barkley_canyon_test_multiclass'):\n",
    "    map_to_ketos_annot_std ={'sound_id_species': 'label'} \n",
    "    #Standardize annotation table format\n",
    "    annot, label_dict = sl.standardize(annot_df, return_label_dict=True, mapper=map_to_ketos_annot_std, trim_table=True)\n",
    "else:    \n",
    "    #Standardize annotation table format\n",
    "    annot, label_dict = sl.standardize(annot_df, return_label_dict=True, trim_table=True)\n",
    "\n",
    "segmented_annot = sl.segment_files(table=target_files_with_len, length=segment_length, step=segment_step, pad=True)\n",
    "\n",
    "# Resetting index to change the multi-indexed dataframe to normal columns\n",
    "annot=annot.reset_index()\n",
    "del annot['annot_id']\n",
    "segmented_annot=segmented_annot.reset_index()\n",
    "del segmented_annot['sel_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8920c1-766e-4fcd-bbbd-f3bc65a041ed",
   "metadata": {},
   "source": [
    "# Save annotations of each file in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fc8db-c926-45e9-b342-8a9842132310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save annotations of each file in dictionary\n",
    "annot_by_file_dict=extract_annot_by_file(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2728bb-f36b-4241-a20f-0e802716cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict # check the label mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5892e79c-e6c4-44fa-8a9e-a4c95b4cfb17",
   "metadata": {},
   "source": [
    "# Prepare list of accepted strings for each class for the specific dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7650cdd-fc58-46b7-99e0-3c0b2855282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_labels=[]\n",
    "hb_labels=[]\n",
    "dolphin_labels=[]\n",
    "other_labels=[]\n",
    "\n",
    "# {'BACKGROUND': 1, 'KW': 2, 'NN': 3, 'Repeat': 4, 'UN': 5}\n",
    "if(file_name_except_extension=='jasco_boundary_pass'): \n",
    "    kw_labels.append(2)\n",
    "    other_labels.append(1)\n",
    "    \n",
    "# {'FS': 1, 'HW': 2, 'HW/KW?': 3, 'KW': 4, 'KW/PWSD?': 5, 'KW?': 6, 'Noise': 7, 'PWSD': 8, 'Sonar': 9, 'UN': 10, 'Vessel Noise': 11}\n",
    "elif(file_name_except_extension=='jasco_roberts_bank'):\n",
    "    kw_labels.extend([4, 6])\n",
    "    hb_labels.extend([2])\n",
    "    dolphin_labels.extend([8])\n",
    "    other_labels.extend([1, 7, 11])\n",
    "    \n",
    "elif(file_name_except_extension=='onc_barkley_canyon'):\n",
    "    kw_labels.extend([4, 6])\n",
    "    hb_labels.extend([3])\n",
    "    dolphin_labels.extend([11, 13])\n",
    "    other_labels.extend([17, 18, 19, 20, 21, 22])\n",
    "    \n",
    "elif(file_name_except_extension=='orcasound'):\n",
    "    kw_labels.extend([2])\n",
    "    \n",
    "elif(file_name_except_extension=='superpod_lime_kiln'):\n",
    "    kw_labels.extend([1])\n",
    "    other_labels.extend([1, 4, 5])\n",
    "    \n",
    "# {'hb': 1, 'kw': 2, 'other': 3}\n",
    "elif(file_name_except_extension=='jasco_malahat_vfpa_test_multiclass'): \n",
    "    kw_labels.append(2)\n",
    "    hb_labels.append(1)\n",
    "    other_labels.append(3)\n",
    "    \n",
    "# {'D': 1, 'HB': 2, 'KW': 3}\n",
    "elif(file_name_except_extension=='onc_barkley_canyon_test_multiclass'): \n",
    "    kw_labels.append(3)\n",
    "    hb_labels.append(2)\n",
    "    dolphin_labels.append(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adee0086-3c83-4783-a7bb-d49b8797ad42",
   "metadata": {},
   "source": [
    "# Populate the segmented annotation table with appropriate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67212a3-bd14-47a1-acab-823a84125835",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_annot['label'] = pd.Series(dtype='object')\n",
    "\n",
    "# Populate the segmented annotation table with appropriate label\n",
    "for index, row in segmented_annot.iterrows():\n",
    "    annot_filtered_by_filename_df=annot_by_file_dict[row['filename']]\n",
    "    # Assign label 0 => Other/Background class label, 1 => KW, 2 => HB, and 3 => Dolphin\n",
    "    # {0: 'OTHER', 1: 'KW', 2: 'HB', 3: 'D'}\n",
    "    if(len(other_labels)!=0):\n",
    "        other_overlap_annot_result=check_overlap(row['start'], row['end'], annot_filtered_by_filename_df, other_labels)\n",
    "        if(other_overlap_annot_result):\n",
    "            segmented_annot.at[index, 'label']=0\n",
    "    \n",
    "    if(len(kw_labels)!=0):\n",
    "        kw_overlap_annot_result=check_overlap(row['start'], row['end'], annot_filtered_by_filename_df, kw_labels)\n",
    "        if(kw_overlap_annot_result):\n",
    "            segmented_annot.at[index, 'label']=1\n",
    "            \n",
    "    if(len(hb_labels)!=0):\n",
    "        hb_overlap_annot_result=check_overlap(row['start'], row['end'], annot_filtered_by_filename_df, hb_labels)\n",
    "        if(hb_overlap_annot_result):\n",
    "            segmented_annot.at[index, 'label']=2\n",
    "    \n",
    "    # this if-block for four classes detector (if Dolphin class is included)\n",
    "    if(number_of_classes==4 and len(dolphin_labels)!=0):\n",
    "        dolphin_overlap_annot_result=check_overlap(row['start'], row['end'], annot_filtered_by_filename_df, dolphin_labels)\n",
    "        if(dolphin_overlap_annot_result):\n",
    "            segmented_annot.at[index, 'label']=3\n",
    "            \n",
    "    # Consider all the remaining labels to the other/background class\n",
    "    if(pd.isnull(segmented_annot['label'][index])==True):\n",
    "        segmented_annot.at[index, 'label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f633e-93a9-48a6-998a-91ec6da8a2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_annot['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309fb89b-e206-4445-a4cc-d32f6b594e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_detections_df['predicted_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2302301-0698-4f28-aa6e-6f0f2d6a387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(segmented_annot['filename'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57d6ae-16f0-460e-933f-46f411b2587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_detections_df['filename'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b781cf6-877a-4604-9eb9-c7f96e643ac0",
   "metadata": {},
   "source": [
    "# Validate if the annotation and model detection dataframes are in the same sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5009c016-54d4-4db2-97c3-7695c28d4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_annot=segmented_annot.sort_values(['filename', 'start'], ascending=[True, True])\n",
    "model_detections_df=model_detections_df.sort_values(['filename', 'start'], ascending=[True, True])\n",
    "validate_annot_detections_serial(segmented_annot, model_detections_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3d1cc0-85e2-43d0-bd99-8e66a7892534",
   "metadata": {},
   "source": [
    "# Calculate performance metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d54a3-0813-4e55-867c-e41855c12e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def calculate_performance_metrics(y_true, y_pred):\n",
    "    accuracy=accuracy_score(y_true, y_pred)\n",
    "    precision=precision_score(y_true, y_pred, average='weighted')\n",
    "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
    "    recall=recall_score(y_true, y_pred, average='weighted') \n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    return accuracy, precision, recall, f1Score, cm\n",
    "\n",
    "def calculate_list_average(value_list):\n",
    "    return sum(value_list)/len(value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a8d3f-5c4e-49ae-be08-c51cfcf24968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance\n",
    "if(number_of_classes==4):\n",
    "    target_names = ['Other', 'KW', 'HB', 'D']\n",
    "elif(number_of_classes==3):\n",
    "    target_names = ['Other', 'KW', 'HB']\n",
    "    \n",
    "y_true=segmented_annot['label'].values\n",
    "y_pred=model_detections_df['predicted_label'].values\n",
    "\n",
    "print(classification_report(list(y_true), list(y_pred), target_names=target_names))\n",
    "accuracy, precision, recall, f1Score, cnf_matrix=calculate_performance_metrics(list(y_true), list(y_pred))\n",
    "print(\"Accuracy  : {}\".format(accuracy))\n",
    "print(\"Precision : {}\".format(precision))\n",
    "print(\"Recall : {}\".format(recall))\n",
    "print(\"f1Score : {}\".format(f1Score))\n",
    "\n",
    "print(\"Confusion matrix:\", cnf_matrix)\n",
    "\n",
    "FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
    "FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "TP = np.diag(cnf_matrix)\n",
    "TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "\n",
    "FP = FP.astype(float)\n",
    "FN = FN.astype(float)\n",
    "TP = TP.astype(float)\n",
    "TN = TN.astype(float)\n",
    "\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "\n",
    "print(\"Specificity or true negative rate:\", calculate_list_average(TNR))\n",
    "print(\"Fall out or false positive rate:\", calculate_list_average(FPR))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
